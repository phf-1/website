#!/usr/bin/env python3

import os
import shutil
import subprocess
import logging
from pathlib import Path
import filecmp
from concurrent.futures import ProcessPoolExecutor
from queue import Queue
import logging.handlers

# Set up a queue-based logging handler for process-safe logging
log_queue = Queue(-1)
queue_handler = logging.handlers.QueueHandler(log_queue)
stream_handler = logging.StreamHandler()
listener = logging.handlers.QueueListener(log_queue, stream_handler)

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(processName)s | %(levelname)s | %(message)s', handlers=[queue_handler])
logger = logging.getLogger()

def get_ids(directory: Path) -> set:
    """Get set of IDs (subdirectory names) in the given directory."""
    return {d.name for d in directory.iterdir() if d.is_dir()}

def remove_obsolete_ids(website_content: Path, source_ids: set):
    """Remove directories in website_content that don't exist in source_ids."""
    website_ids = get_ids(website_content / 'public') | get_ids(website_content / 'private')
    logger.debug(f"Website IDs: {website_ids}")
    for id in website_ids - source_ids:
        for section in ['public', 'private']:
            dir_path = website_content / section / id
            if dir_path.exists():
                shutil.rmtree(dir_path)
                logger.info(f"Removed obsolete directory: {dir_path}")

def rsync_directories(source: Path, website: Path):
    """Synchronize source to website directory, ignoring article.html files."""
    for section in ['public', 'private']:
        source_section = source / section
        website_section = website / section
        if not source_section.exists():
            continue
        website_section.mkdir(parents=True, exist_ok=True)
        # Use rsync command to copy, excluding article.html
        cmd = [
            'rsync', '-a', str(source_section) + '/', str(website_section) + '/'
        ]
        logger.debug(f"rsync command: {cmd}")
        try:
            subprocess.run(cmd, check=True, capture_output=True, text=True)
            logger.info(f"Synchronized {source_section} to {website_section}")
        except subprocess.CalledProcessError as e:
            logger.error(f"rsync failed: {e.stderr}")
            raise

def build_html(org_path: Path) -> tuple[str, str]:
    """Convert Org file to HTML using Emacs."""
    html_path = org_path.with_suffix('.html')
    logger.debug(f"Building HTML for: {html_path}")
    try:
        # Ensure Emacs can find org-mode
        cmd = [
            'emacs', '-Q', '--batch',
            '--file', str(org_path),
            '--eval', '(setq org-export-with-broken-links t)',
            '--eval', '(setq org-export-with-title t)',
            '--eval', '(setq org-html-doctype "html5")',
            '--eval', '(setq org-html-html5-fancy t)',
            '--eval', '(setq org-export-with-section-numbers nil)',
            '--eval', '(setq org-todo-keywords \'((sequence "TODO(t)" "WAITING(w)" "DOING(o)" "|" "DONE(d)" "FAILED(f)" "CANCELED(c)")))',
            '--eval', '(org-html-export-to-html nil nil nil t)'
        ]
        print(cmd)
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(result)
        if html_path.exists():
            return "ok", str(html_path)
        else:
            return "error", f"HTML file {html_path} was not created"
    except subprocess.CalledProcessError as e:
        return "error", f"Emacs failed to convert {org_path}: {e.stderr}"

def path_to_html(path: Path) -> None:
    """Process an Org file to HTML if needed."""
    html_path = path.with_suffix('.html')
    logger.debug(f"Checking HTML: {html_path}")

    # Check conditions for conversion
    if not html_path.exists() or (html_path.stat().st_mtime < path.stat().st_mtime):
        result, msg = build_html(path)
        if result == "error":
            logger.error(msg)
        else:
            logger.info(f"{path} → {html_path}")
    else:
        logger.info(f"{path} → {html_path} (skipped, up to date)")

def source_to_website_content(source_content: str, website_content: str) -> str:
    """Main function to transform source_content to website_content."""
    # Start the queue listener for logging
    listener.start()

    try:
        source_path = Path(source_content).resolve()
        logger.debug(f"Source path: {source_path}")
        website_path = Path(website_content).resolve()
        public = website_path / 'public'
        if not public.is_dir():
            public.mkdir()
        private = website_path / 'private'
        if not private.is_dir():
            private.mkdir()
        logger.debug(f"Website path: {website_path}")

        # Get IDs from source and website
        source_ids = get_ids(source_path / 'public') | get_ids(source_path / 'private')
        logger.debug(f"Source IDs: {source_ids}")

        # Remove obsolete IDs from website_content
        remove_obsolete_ids(website_path, source_ids)

        # Synchronize directories
        rsync_directories(source_path, website_path)

        # Process all article.org files in parallel
        paths = list(website_path.rglob("**/article.org"))
        logger.debug(f"Org files to process: {paths}")
        with ProcessPoolExecutor() as executor:
            executor.map(path_to_html, paths)

        return "ok"
    finally:
        # Stop the queue listener to ensure all logs are flushed
        listener.stop()

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("Usage: source_to_website_content <source_content> <website_content>")
        sys.exit(1)

    result = source_to_website_content(sys.argv[1], sys.argv[2])
    print(result)
